{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import njit\n",
    "\n",
    "data_dir = 'Data/PriceData'\n",
    "\n",
    "# Load Parquet files into data_frames dictionary\n",
    "data_frames = {}\n",
    "files = [file for file in os.listdir(data_dir) if file.endswith('.parquet')]\n",
    "for file in tqdm(files, desc=\"Loading Parquet files\"):\n",
    "    ticker = file.split('.')[0]\n",
    "    data_frames[ticker] = pd.read_parquet(os.path.join(data_dir, file))\n",
    "\n",
    "@njit\n",
    "def calculate_dynamic_weights(volatility, mean_volatility, std_volatility):\n",
    "    if std_volatility == 0:\n",
    "        return np.array([0.25, 0.25, 0.25, 0.25])\n",
    "    \n",
    "    z_score = (volatility - mean_volatility) / std_volatility\n",
    "    weights = np.array([0.25 - 0.1*z_score, 0.25 - 0.05*z_score, 0.25 + 0.05*z_score, 0.25 + 0.1*z_score])\n",
    "    weights = np.maximum(np.minimum(weights, 1), 0)  # This replaces np.clip\n",
    "    return weights / np.sum(weights)\n",
    "\n",
    "@njit\n",
    "def calculate_weighted_returns_fast(returns, volatility, mean_volatility, std_volatility):\n",
    "    weighted_returns = np.zeros(len(returns))\n",
    "    for i in range(21, len(returns)):\n",
    "        weights = calculate_dynamic_weights(volatility[i], mean_volatility[i], std_volatility[i])\n",
    "        weighted_returns[i] = np.sum(returns[i] * weights)\n",
    "    return weighted_returns\n",
    "\n",
    "def process_stock_data(df):\n",
    "    df['Daily_Return'] = df['Close'].pct_change().fillna(method='bfill').round(3)\n",
    "    df['Weekly_Return'] = df['Close'].pct_change(5).fillna(method='bfill').round(3)\n",
    "    df['Monthly_Return'] = df['Close'].pct_change(21).fillna(method='bfill').round(3)\n",
    "    df['Yearly_Return'] = df['Close'].pct_change(252).fillna(method='bfill').round(3)\n",
    "\n",
    "    df['Volatility'] = df['Daily_Return'].rolling(window=21).std().fillna(method='bfill')\n",
    "    df['Mean_Volatility'] = df['Volatility'].rolling(window=21).mean().fillna(method='bfill')\n",
    "    df['Volatility_Std'] = df['Volatility'].rolling(window=21).std().fillna(method='bfill')\n",
    "\n",
    "    returns = np.column_stack((df['Daily_Return'], df['Weekly_Return'], df['Monthly_Return'], df['Yearly_Return']))\n",
    "    volatility = df['Volatility'].values\n",
    "    mean_volatility = df['Mean_Volatility'].values\n",
    "    std_volatility = df['Volatility_Std'].values\n",
    "\n",
    "    df['Weighted_Return'] = calculate_weighted_returns_fast(returns, volatility, mean_volatility, std_volatility)\n",
    "    df['Weighted_Return'] = df['Weighted_Return'].round(3)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Process all stocks\n",
    "for ticker, df in tqdm(data_frames.items(), desc=\"Processing stocks\"):\n",
    "    data_frames[ticker] = process_stock_data(df)\n",
    "\n",
    "returns_df = pd.DataFrame({ticker: df['Weighted_Return'] for ticker, df in data_frames.items()}).fillna(method='bfill')\n",
    "\n",
    "correlation_matrix = returns_df.corr().fillna(method='bfill').fillna(method='ffill')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_correlation = scaler.fit_transform(correlation_matrix)\n",
    "\n",
    "# Perform clustering\n",
    "kmeans = KMeans(n_clusters=8, random_state=0, n_init=10)\n",
    "clusters = kmeans.fit_predict(scaled_correlation)\n",
    "\n",
    "correlation_matrix_df = pd.DataFrame(correlation_matrix)\n",
    "correlation_matrix_df['Cluster'] = clusters\n",
    "\n",
    "# Elbow Method\n",
    "inertia = []\n",
    "for k in tqdm(range(2, 20), desc=\"Elbow Method\"):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
    "    kmeans.fit(scaled_correlation)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(2, 20), inertia, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "plt.show()\n",
    "\n",
    "# Silhouette Score\n",
    "silhouette_scores = []\n",
    "for k in tqdm(range(2, 20), desc=\"Silhouette Score\"):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
    "    labels = kmeans.fit_predict(scaled_correlation)\n",
    "    silhouette_scores.append(silhouette_score(scaled_correlation, labels))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(2, 20), silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Scores for Optimal Number of Clusters')\n",
    "plt.show()\n",
    "\n",
    "clustered_assets = correlation_matrix_df[['Cluster']].reset_index()\n",
    "clustered_assets.columns = ['Ticker', 'Cluster']\n",
    "\n",
    "def calculate_group_correlations_vectorized(correlation_matrix, clustered_assets):\n",
    "    corr_array = correlation_matrix.values\n",
    "    unique_clusters = clustered_assets['Cluster'].unique()\n",
    "    cluster_indices = {cluster: clustered_assets.index[clustered_assets['Cluster'] == cluster].tolist() \n",
    "                       for cluster in unique_clusters}\n",
    "    \n",
    "    result = pd.DataFrame(index=correlation_matrix.index, \n",
    "                          columns=[f'correlation_{cluster}' for cluster in unique_clusters])\n",
    "    \n",
    "    for cluster in tqdm(unique_clusters, desc=\"Calculating Group Correlations\"):\n",
    "        indices = cluster_indices[cluster]\n",
    "        cluster_correlations = corr_array[:, indices].mean(axis=1)\n",
    "        result[f'correlation_{cluster}'] = cluster_correlations\n",
    "    \n",
    "    return result\n",
    "\n",
    "group_correlations = calculate_group_correlations_vectorized(correlation_matrix, clustered_assets)\n",
    "\n",
    "clustered_assets = clustered_assets.merge(group_correlations, left_on='Ticker', right_index=True)\n",
    "\n",
    "mean_intra_group_corr = clustered_assets.groupby('Cluster')[group_correlations.columns].mean().mean(axis=1)\n",
    "clustered_assets['mean_intragroup_correlation'] = clustered_assets['Cluster'].map(mean_intra_group_corr)\n",
    "clustered_assets['diff_to_mean_group_corr'] = clustered_assets.apply(\n",
    "    lambda row: row[f'correlation_{row.Cluster}'] - row['mean_intragroup_correlation'], axis=1)\n",
    "\n",
    "reordered_columns = ['Ticker', 'Cluster', 'mean_intragroup_correlation', 'diff_to_mean_group_corr'] + list(group_correlations.columns)\n",
    "clustered_assets = clustered_assets[reordered_columns].round(5)\n",
    "\n",
    "clustered_assets.to_parquet('Correlations.parquet', index=False)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(clustered_assets[['Ticker', 'Cluster']])\n",
    "\n",
    "print(\"Correlations saved to 'Correlations.parquet'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
