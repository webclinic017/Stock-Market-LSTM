{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Parquet files: 100%|██████████| 5769/5769 [00:08<00:00, 700.82it/s]\n",
      "Calculating Returns: 100%|██████████| 5769/5769 [00:01<00:00, 4229.81it/s]\n",
      "c:\\Users\\Masam\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "Elbow Method:   0%|          | 0/4 [00:00<?, ?it/s]c:\\Users\\Masam\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "Elbow Method:  25%|██▌       | 1/4 [00:02<00:06,  2.06s/it]c:\\Users\\Masam\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "Elbow Method:  50%|█████     | 2/4 [00:04<00:04,  2.44s/it]c:\\Users\\Masam\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "Elbow Method:  75%|███████▌  | 3/4 [00:09<00:03,  3.38s/it]c:\\Users\\Masam\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "Elbow Method: 100%|██████████| 4/4 [00:13<00:00,  3.46s/it]\n",
      "Silhouette Score:   0%|          | 0/4 [00:00<?, ?it/s]c:\\Users\\Masam\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "Silhouette Score:  25%|██▌       | 1/4 [00:02<00:07,  2.64s/it]c:\\Users\\Masam\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "Silhouette Score:  50%|█████     | 2/4 [00:06<00:06,  3.08s/it]c:\\Users\\Masam\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "Silhouette Score:  75%|███████▌  | 3/4 [00:10<00:03,  3.93s/it]c:\\Users\\Masam\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "Silhouette Score: 100%|██████████| 4/4 [00:16<00:00,  4.04s/it]\n",
      "Calculating Group Correlations:   0%|          | 0/5769 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import multiprocessing as mp\n",
    "\n",
    "data_dir = 'Data/PriceData'\n",
    "\n",
    "# Load Parquet files into data_frames dictionary\n",
    "data_frames = {}\n",
    "files = [file for file in os.listdir(data_dir) if file.endswith('.parquet')]\n",
    "for file in tqdm(files, desc=\"Loading Parquet files\"):\n",
    "    ticker = file.split('.')[0]\n",
    "    data_frames[ticker] = pd.read_parquet(os.path.join(data_dir, file))\n",
    "\n",
    "def calculate_returns(df):\n",
    "    df['Return'] = df['Close'].pct_change(fill_method=None)\n",
    "    return df\n",
    "\n",
    "def resample_and_calculate_returns(df, frequency):\n",
    "    resampled_df = df.resample(frequency).last()\n",
    "    resampled_df['Return'] = resampled_df['Close'].pct_change(fill_method=None)\n",
    "    return resampled_df\n",
    "\n",
    "for ticker, df in tqdm(data_frames.items(), desc=\"Calculating Returns\"):\n",
    "    data_frames[ticker] = calculate_returns(df)\n",
    "\n",
    "# Calculate returns for different timeframes\n",
    "def create_returns_df(data_frames, frequency):\n",
    "    return pd.DataFrame({ticker: resample_and_calculate_returns(df, frequency)['Return'] for ticker, df in data_frames.items()}).dropna(how='all')\n",
    "\n",
    "daily_returns_df = create_returns_df(data_frames, 'D')\n",
    "weekly_returns_df = create_returns_df(data_frames, 'W')\n",
    "monthly_returns_df = create_returns_df(data_frames, 'M')\n",
    "\n",
    "# Calculate correlation matrices for different timeframes\n",
    "def calculate_correlation_matrix(returns_df):\n",
    "    return returns_df.corr()\n",
    "\n",
    "daily_correlation_matrix = calculate_correlation_matrix(daily_returns_df)\n",
    "weekly_correlation_matrix = calculate_correlation_matrix(weekly_returns_df)\n",
    "monthly_correlation_matrix = calculate_correlation_matrix(monthly_returns_df)\n",
    "\n",
    "#===============================[ Clustering TimeFrame Weighting ]==================================#\n",
    "weights = {'daily': 0.65, 'weekly': 0.25, 'monthly': 0.10}\n",
    "weighted_correlation_matrix = (weights['daily'] * daily_correlation_matrix +\n",
    "                               weights['weekly'] * weekly_correlation_matrix +\n",
    "                               weights['monthly'] * monthly_correlation_matrix)\n",
    "\n",
    "# Handle missing values and scale the weighted correlation matrix\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputed_correlation = imputer.fit_transform(weighted_correlation_matrix)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_correlation = scaler.fit_transform(imputed_correlation)\n",
    "\n",
    "# Perform clustering\n",
    "kmeans = KMeans(n_clusters=6, random_state=0)\n",
    "clusters = kmeans.fit_predict(scaled_correlation)\n",
    "\n",
    "# Create a DataFrame for the correlation matrix with cluster labels\n",
    "correlation_matrix_df = pd.DataFrame(weighted_correlation_matrix)\n",
    "correlation_matrix_df['Cluster'] = clusters\n",
    "\n",
    "# Elbow Method to determine the optimal number of clusters\n",
    "def calculate_elbow_method(scaled_correlation):\n",
    "    inertia = []\n",
    "    for k in tqdm(range(2, 6), desc=\"Elbow Method\"):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans.fit(scaled_correlation)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    return inertia\n",
    "\n",
    "inertia = calculate_elbow_method(scaled_correlation)\n",
    "\n",
    "# Silhouette Scores for optimal number of clusters\n",
    "def calculate_silhouette_scores(scaled_correlation):\n",
    "    silhouette_scores = []\n",
    "    for k in tqdm(range(2, 6), desc=\"Silhouette Score\"):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        labels = kmeans.fit_predict(scaled_correlation)\n",
    "        silhouette_scores.append(silhouette_score(scaled_correlation, labels))\n",
    "    return silhouette_scores\n",
    "\n",
    "silhouette_scores = calculate_silhouette_scores(scaled_correlation)\n",
    "\n",
    "# Calculate group correlations\n",
    "clustered_assets = correlation_matrix_df[['Cluster']].reset_index()\n",
    "clustered_assets.columns = ['Ticker', 'Cluster']\n",
    "\n",
    "\n",
    "\n",
    "##===============================[ Calculate Group Correlations ]==================================#\n",
    "##===============================[ Calculate Group Correlations ]==================================#\n",
    "##===============================[ Calculate Group Correlations ]==================================#\n",
    "\n",
    "##===============================[ Calculate Group Correlations ]==================================#\n",
    "\n",
    "##===============================[ Calculate Group Correlations ]==================================#\n",
    "\n",
    "##===============================[ Calculate Group Correlations ]==================================#\n",
    "\n",
    "##===============================[ Calculate Group Correlations ]==================================#\n",
    "\n",
    "##===============================[ Calculate Group Correlations ]==================================#\n",
    "\n",
    "def calculate_group_correlations_for_ticker(ticker):\n",
    "    ##get what group each ticker is in\n",
    "    group = clustered_assets.loc[clustered_assets['Ticker'] == ticker, 'Cluster'].values[0]\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "group_corr_columns = [f'correlation_{group}' for group in sorted(clustered_assets['Cluster'].unique())]\n",
    "for col in group_corr_columns:\n",
    "    clustered_assets[col] = np.nan\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tickers = clustered_assets['Ticker'].unique()\n",
    "    \n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        results = list(tqdm(pool.imap(calculate_group_correlations_for_ticker, tickers), total=len(tickers), desc=\"Calculating Group Correlations\"))\n",
    "\n",
    "    for ticker, group_correlations in results:\n",
    "        for col, value in group_correlations.items():\n",
    "            clustered_assets.loc[clustered_assets['Ticker'] == ticker, col] = value\n",
    "\n",
    "    mean_intra_group_corr = clustered_assets.groupby('Cluster')[group_corr_columns].mean().mean(axis=1)\n",
    "    clustered_assets['mean_intragroup_correlation'] = clustered_assets['Cluster'].map(mean_intra_group_corr)\n",
    "    clustered_assets['diff_to_mean_group_corr'] = clustered_assets.apply(lambda row: row[f'correlation_{row.Cluster}'] - row['mean_intragroup_correlation'], axis=1)\n",
    "\n",
    "    reordered_columns = ['Ticker', 'Cluster', 'mean_intragroup_correlation', 'diff_to_mean_group_corr'] + group_corr_columns\n",
    "    clustered_assets = clustered_assets[reordered_columns]\n",
    "\n",
    "    clustered_assets = clustered_assets.round(5)\n",
    "    clustered_assets.to_parquet('Correlations.parquet', index=False)\n",
    "\n",
    "print(\"Correlations saved to 'Correlations.parquet'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
