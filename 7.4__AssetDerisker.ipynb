{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Parquet files: 100%|██████████| 1450/1450 [00:01<00:00, 780.81it/s]\n",
      "Calculating Returns: 100%|██████████| 1450/1450 [00:00<00:00, 4667.58it/s]\n",
      "Elbow Method: 100%|██████████| 18/18 [00:01<00:00, 11.66it/s]\n",
      "Silhouette Score: 100%|██████████| 18/18 [00:01<00:00,  9.02it/s]\n",
      "Calculating Group Correlations: 100%|██████████| 1450/1450 [00:00<00:00, 9224.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlations saved to 'Correlations.parquet'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "\n",
    "data_dir = 'Data/PriceData'\n",
    "\n",
    "# Load Parquet files into data_frames dictionary\n",
    "data_frames = {}\n",
    "files = [file for file in os.listdir(data_dir) if file.endswith('.parquet')]\n",
    "for file in tqdm(files, desc=\"Loading Parquet files\"):\n",
    "    ticker = file.split('.')[0]\n",
    "    data_frames[ticker] = pd.read_parquet(os.path.join(data_dir, file))\n",
    "\n",
    "def calculate_returns(df):\n",
    "    df['Return'] = df['Close'].pct_change(fill_method=None)\n",
    "    return df\n",
    "\n",
    "def resample_and_calculate_returns(df, frequency):\n",
    "    resampled_df = df.resample(frequency).last()\n",
    "    resampled_df['Return'] = resampled_df['Close'].pct_change(fill_method=None)\n",
    "    return resampled_df\n",
    "\n",
    "for ticker, df in tqdm(data_frames.items(), desc=\"Calculating Returns\"):\n",
    "    data_frames[ticker] = calculate_returns(df)\n",
    "\n",
    "# Calculate returns for different timeframes\n",
    "daily_returns_df = pd.DataFrame({ticker: df['Return'] for ticker, df in data_frames.items()}).dropna(how='all')\n",
    "weekly_returns_df = pd.DataFrame({ticker: resample_and_calculate_returns(df, 'W')['Return'] for ticker, df in data_frames.items()}).dropna(how='all')\n",
    "monthly_returns_df = pd.DataFrame({ticker: resample_and_calculate_returns(df, 'ME')['Return'] for ticker, df in data_frames.items()}).dropna(how='all')\n",
    "\n",
    "# Calculate correlation matrices for different timeframes\n",
    "daily_correlation_matrix = daily_returns_df.corr()\n",
    "weekly_correlation_matrix = weekly_returns_df.corr()\n",
    "monthly_correlation_matrix = monthly_returns_df.corr()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#===============================[ Clustering TimeFrame Weighting ]==================================#\n",
    "#===============================[ Clustering TimeFrame Weighting ]==================================#\n",
    "#===============================[ Clustering TimeFrame Weighting ]==================================#\n",
    "weights = {'daily': 0.65, 'weekly': 0.25, 'monthly': 0.10}\n",
    "weighted_correlation_matrix = (weights['daily'] * daily_correlation_matrix +\n",
    "                               weights['weekly'] * weekly_correlation_matrix +\n",
    "                               weights['monthly'] * monthly_correlation_matrix)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Handle missing values and scale the weighted correlation matrix\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputed_correlation = imputer.fit_transform(weighted_correlation_matrix)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_correlation = scaler.fit_transform(imputed_correlation)\n",
    "\n",
    "# Perform clustering\n",
    "kmeans = KMeans(n_clusters=6, random_state=0)\n",
    "clusters = kmeans.fit_predict(scaled_correlation)\n",
    "\n",
    "# Create a DataFrame for the correlation matrix with cluster labels\n",
    "correlation_matrix_df = pd.DataFrame(weighted_correlation_matrix)\n",
    "correlation_matrix_df['Cluster'] = clusters\n",
    "\n",
    "# Elbow Method to determine the optimal number of clusters\n",
    "inertia = []\n",
    "for k in tqdm(range(2, 20), desc=\"Elbow Method\"):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(scaled_correlation)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Silhouette Scores for optimal number of clusters\n",
    "silhouette_scores = []\n",
    "for k in tqdm(range(2, 20), desc=\"Silhouette Score\"):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    labels = kmeans.fit_predict(scaled_correlation)\n",
    "    silhouette_scores.append(silhouette_score(scaled_correlation, labels))\n",
    "\n",
    "# Calculate group correlations\n",
    "clustered_assets = correlation_matrix_df[['Cluster']].reset_index()\n",
    "clustered_assets.columns = ['Ticker', 'Cluster']\n",
    "\n",
    "def calculate_group_correlations(ticker):\n",
    "    group_correlations = {}\n",
    "    for group in sorted(clustered_assets['Cluster'].unique()):  # Ensure the clusters are processed in order\n",
    "        tickers_in_group = clustered_assets[clustered_assets['Cluster'] == group]['Ticker']\n",
    "        group_corr = weighted_correlation_matrix.loc[ticker, tickers_in_group].mean()\n",
    "        group_correlations[f'correlation_{group}'] = group_corr\n",
    "    return ticker, group_correlations\n",
    "\n",
    "group_corr_columns = [f'correlation_{group}' for group in sorted(clustered_assets['Cluster'].unique())]\n",
    "for col in group_corr_columns:\n",
    "    clustered_assets[col] = np.nan\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tickers = clustered_assets['Ticker'].unique()\n",
    "    \n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        results = list(tqdm(pool.imap(calculate_group_correlations, tickers), total=len(tickers), desc=\"Calculating Group Correlations\"))\n",
    "\n",
    "    for ticker, group_correlations in results:\n",
    "        for col, value in group_correlations.items():\n",
    "            clustered_assets.loc[clustered_assets['Ticker'] == ticker, col] = value\n",
    "\n",
    "    mean_intra_group_corr = clustered_assets.groupby('Cluster')[group_corr_columns].mean().mean(axis=1)\n",
    "    clustered_assets['mean_intragroup_correlation'] = clustered_assets['Cluster'].map(mean_intra_group_corr)\n",
    "    clustered_assets['diff_to_mean_group_corr'] = clustered_assets.apply(lambda row: row[f'correlation_{row.Cluster}'] - row['mean_intragroup_correlation'], axis=1)\n",
    "\n",
    "    reordered_columns = ['Ticker', 'Cluster', 'mean_intragroup_correlation', 'diff_to_mean_group_corr'] + group_corr_columns\n",
    "    clustered_assets = clustered_assets[reordered_columns]\n",
    "\n",
    "    clustered_assets = clustered_assets.round(5)\n",
    "    clustered_assets.to_parquet('Correlations.parquet', index=False)\n",
    "\n",
    "print(\"Correlations saved to 'Correlations.parquet'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
