{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_139186/1772015435.py:96: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "/root/root/miniconda4/envs/tf/lib/python3.9/site-packages/numpy/lib/function_base.py:4573: RuntimeWarning: invalid value encountered in subtract\n",
      "  diff_b_a = subtract(b, a)\n",
      "/tmp/ipykernel_139186/1772015435.py:96: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_139186/1772015435.py:96: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_139186/1772015435.py:96: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_139186/1772015435.py:96: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_139186/1772015435.py:96: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_139186/1772015435.py:96: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_139186/1772015435.py:96: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_139186/1772015435.py:96: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_139186/1772015435.py:96: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_139186/1772015435.py:96: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_139186/1772015435.py:96: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_139186/1772015435.py:96: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_139186/1772015435.py:96: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_139186/1772015435.py:96: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Data Manipulation and Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Machine Learning and Data Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Parallel and Asynchronous Programming\n",
    "import multiprocessing\n",
    "import asyncio\n",
    "import joblib\n",
    "\n",
    "# Miscellaneous\n",
    "from scipy import signal\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.input_dir = \"Data/IndicatorData\"\n",
    "        self.output_dir = \"Data/ScaledData\"\n",
    "        self.log_file = \"Data/ScaledData/_ScalingErrors.log\"\n",
    "        self.scaling_methods_file = \"__ScalingMethods.csv\"  # Add this line\n",
    "\n",
    "    def setup_logging(self):\n",
    "        logging.basicConfig(filename=self.log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        logging.info(f\"Logging started at {datetime.datetime.now()}\")\n",
    "\n",
    "###===================================( Data Preprocessing )===================================###\n",
    "###===================================( Data Preprocessing )===================================###\n",
    "###===================================( Data Preprocessing )===================================###\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_scaling_methods(config):\n",
    "    scaling_methods_file = config.scaling_methods_file  # File in the workspace folder\n",
    "    try:\n",
    "        scaling_methods_df = pd.read_csv(scaling_methods_file)\n",
    "        scaling_methods = {}\n",
    "        for _, row in scaling_methods_df.iterrows():\n",
    "            scaling_methods[row['Column']] = {\n",
    "                'ScalingMethod': row['ScalingMethod'],\n",
    "                'LogTransform': row['LogTransform'] == 'True',\n",
    "                'Detrend': row['Detrend']\n",
    "            }\n",
    "        logging.info(\"Successfully loaded scaling methods.\")\n",
    "        return scaling_methods\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading scaling methods: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "\n",
    "def outlyer_squasher(df, percentile1=0.999, percentile2=0.001):\n",
    "    try:\n",
    "        for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "            lower_quantile = df[col].quantile(percentile2)  # Get the lower quantile\n",
    "            upper_quantile = df[col].quantile(percentile1)  # Get the upper quantile\n",
    "            df[col] = df[col].clip(lower=lower_quantile, upper=upper_quantile)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error squashing outliers: {e}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def handle_inf_and_scale_dynamic(df, scaling_methods, window_ratio=0.1, min_window_size=100):\n",
    "    try:\n",
    "        # Replace inf/-inf with NaN\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "        # Forward fill to handle NaNs\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        window_size = max(int(len(df) * window_ratio), min_window_size)\n",
    "        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "        for start in range(0, len(df), window_size):\n",
    "            end = min(start + window_size, len(df))\n",
    "            df_subset = df.iloc[start:end]\n",
    "            numeric_cols = df_subset.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "            for col in numeric_cols:\n",
    "                # Check if column has a specified scaling method\n",
    "                if col in scaling_methods:\n",
    "                    # Log transform if specified\n",
    "                    if scaling_methods[col]['LogTransform']:\n",
    "                        df = apply_log_transform(df, col)\n",
    "\n",
    "                    # Detrend if specified\n",
    "                    if scaling_methods[col]['Detrend']:\n",
    "                        df = detrend_data(df, col, scaling_methods[col]['Detrend'])\n",
    "\n",
    "                scaler = get_scaler(scaling_methods[col]['ScalingMethod'])\n",
    "\n",
    "        logging.info(\"Dataframe dynamic scaling with inf handling complete.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in dynamic scaling of dataframe: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_scaler(scaling_method):\n",
    "    if scaling_method == 'StandardScaler':\n",
    "        return StandardScaler()\n",
    "    elif scaling_method == 'MinMaxScaler':\n",
    "        return MinMaxScaler()\n",
    "    elif scaling_method == 'RobustScaler':\n",
    "        return RobustScaler()\n",
    "    else:\n",
    "        logging.error(f\"No scaling method needed\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def apply_log_transform(df, column):\n",
    "    try:\n",
    "        df[column] = np.log1p(df[column])\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error applying log transform to column {column}: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def detrend_data(df, column, method):\n",
    "\n",
    "\n",
    "    try:\n",
    "        if method == 'Differencing':\n",
    "            df[column] = df[column].diff().fillna(df[column])\n",
    "        elif method == 'ScipyDetrend':\n",
    "            df[column] = signal.detrend(df[column])\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error detrending column {column}: {e}\")\n",
    "    return df\n",
    "\n",
    "def interpolated(df):\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    numeric_df = numeric_df.interpolate(method='linear', limit_direction='both')\n",
    "    df[numeric_df.columns] = numeric_df\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_dataframe(df, file_path):\n",
    "    config = Config()  # Instantiate a Config object\n",
    "    file_name = file_path.split(\"/\")[-1].split(\"_\")[0] + \"_scaled.csv\"\n",
    "    file_path = os.path.join(config.output_dir, file_name)  # Use the instance's output_dir\n",
    "    df.to_csv(file_path, index=False)\n",
    "    logging.info(f\"File {file_name} has been saved to {config.output_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_dataframe(df, file_path, scaling_methods):\n",
    "    df = outlyer_squasher(df)\n",
    "    df = interpolated(df)\n",
    "    df = handle_inf_and_scale_dynamic(df, scaling_methods)\n",
    "    df = interpolated(df)\n",
    "    save_dataframe(df, file_path)\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = Config()\n",
    "    config.setup_logging()\n",
    "\n",
    "    # Load the scaling methods\n",
    "    scaling_methods = load_scaling_methods(config)\n",
    "\n",
    "    # Clear all files ending in .csv in the output directory\n",
    "    for file in os.listdir(config.output_dir):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(config.output_dir, file)\n",
    "            os.remove(file_path)\n",
    "    logging.info(f\"Files in {config.output_dir} have been cleared.\")\n",
    "\n",
    "    for file in os.listdir(config.input_dir):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(config.input_dir, file)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                process_dataframe(df, file_path, scaling_methods)\n",
    "                # Optionally, save processed data to the output directory\n",
    "                ##df.to_csv(os.path.join(config.output_dir, file), index=False)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
