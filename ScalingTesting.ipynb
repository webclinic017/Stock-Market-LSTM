{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   1%|‚ñè         | 25/1761 [00:14<16:15,  1.78it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 239\u001b[0m\n\u001b[1;32m    236\u001b[0m                 logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 239\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 234\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    230\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m too short to process.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 234\u001b[0m     \u001b[43mprocess_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaling_methods\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    236\u001b[0m     logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 192\u001b[0m, in \u001b[0;36mprocess_dataframe\u001b[0;34m(df, file_path, scaling_methods)\u001b[0m\n\u001b[1;32m    190\u001b[0m df \u001b[38;5;241m=\u001b[39m outlyer_squasher(df)\n\u001b[1;32m    191\u001b[0m df \u001b[38;5;241m=\u001b[39m interpolated(df)\n\u001b[0;32m--> 192\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mhandle_inf_and_scale_dynamic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaling_methods\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m df \u001b[38;5;241m=\u001b[39m interpolated(df)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "Cell \u001b[0;32mIn[1], line 120\u001b[0m, in \u001b[0;36mhandle_inf_and_scale_dynamic\u001b[0;34m(df, scaling_methods, window_ratio, min_window_size)\u001b[0m\n\u001b[1;32m    118\u001b[0m scaler \u001b[38;5;241m=\u001b[39m get_scaler(scaling_method)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     df_subset[col] \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdf_subset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfillna(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad\u001b[39m\u001b[38;5;124m'\u001b[39m))  \u001b[38;5;66;03m# scale and handle NaNs\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     df_subset[col] \u001b[38;5;241m=\u001b[39m df_subset[col]\u001b[38;5;241m.\u001b[39mfillna(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpad\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/root/miniconda4/envs/tf/lib/python3.9/site-packages/pandas/core/frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/root/miniconda4/envs/tf/lib/python3.9/site-packages/pandas/core/indexes/base.py:6195\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6192\u001b[0m     keyarr \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(keyarr)\n\u001b[1;32m   6194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 6195\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6196\u001b[0m     keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreindex(keyarr)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/root/miniconda4/envs/tf/lib/python3.9/site-packages/pandas/core/indexes/base.py:6182\u001b[0m, in \u001b[0;36mIndex.get_indexer_for\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m   6164\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6165\u001b[0m \u001b[38;5;124;03mGuaranteed return of an indexer even when non-unique.\u001b[39;00m\n\u001b[1;32m   6166\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6179\u001b[0m \u001b[38;5;124;03marray([0, 2])\u001b[39;00m\n\u001b[1;32m   6180\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[0;32m-> 6182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6183\u001b[0m indexer, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(target)\n\u001b[1;32m   6184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexer\n",
      "File \u001b[0;32m~/root/miniconda4/envs/tf/lib/python3.9/site-packages/pandas/core/indexes/base.py:3880\u001b[0m, in \u001b[0;36mIndex.get_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3878\u001b[0m method \u001b[38;5;241m=\u001b[39m clean_reindex_fill_method(method)\n\u001b[1;32m   3879\u001b[0m orig_target \u001b[38;5;241m=\u001b[39m target\n\u001b[0;32m-> 3880\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_cast_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_method(method, limit, tolerance)\n\u001b[1;32m   3884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n",
      "File \u001b[0;32m~/root/miniconda4/envs/tf/lib/python3.9/site-packages/pandas/core/indexes/base.py:6683\u001b[0m, in \u001b[0;36mIndex._maybe_cast_listlike_indexer\u001b[0;34m(self, target)\u001b[0m\n\u001b[1;32m   6679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_cast_listlike_indexer\u001b[39m(\u001b[38;5;28mself\u001b[39m, target) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[1;32m   6680\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6681\u001b[0m \u001b[38;5;124;03m    Analogue to maybe_cast_indexer for get_indexer instead of get_loc.\u001b[39;00m\n\u001b[1;32m   6682\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 6683\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mensure_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/root/miniconda4/envs/tf/lib/python3.9/site-packages/pandas/core/indexes/base.py:7649\u001b[0m, in \u001b[0;36mensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   7647\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Index(index_like, copy\u001b[38;5;241m=\u001b[39mcopy, tupleize_cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   7648\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 7649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/root/miniconda4/envs/tf/lib/python3.9/site-packages/pandas/core/indexes/base.py:565\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[0;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[1;32m    562\u001b[0m         data \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(data, dtype\u001b[38;5;241m=\u001b[39m_dtype_obj)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex must be specified when data is not list-like\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/root/miniconda4/envs/tf/lib/python3.9/site-packages/pandas/core/construction.py:606\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    604\u001b[0m subarr \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m--> 606\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_infer_to_datetimelike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    608\u001b[0m         object_index\n\u001b[1;32m    609\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m using_pyarrow_string_dtype()\n\u001b[1;32m    610\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m is_string_dtype(subarr)\n\u001b[1;32m    611\u001b[0m     ):\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;66;03m# Avoid inference when string option is set\u001b[39;00m\n\u001b[1;32m    613\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m~/root/miniconda4/envs/tf/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1190\u001b[0m, in \u001b[0;36mmaybe_infer_to_datetimelike\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# error: Incompatible return value type (got \"Union[ExtensionArray,\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# ndarray[Any, Any]]\", expected \"Union[ndarray[Any, Any], DatetimeArray,\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# TimedeltaArray, PeriodArray, IntervalArray]\")\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_convert_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Here we do not convert numeric dtypes, as if we wanted that,\u001b[39;49;00m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#  numpy would have done it for us.\u001b[39;49;00m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_non_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_if_all_nat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mM8[ns]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32mlib.pyx:2728\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/root/miniconda4/envs/tf/lib/python3.9/site-packages/pandas/_config/__init__.py:56\u001b[0m, in \u001b[0;36musing_pyarrow_string_dtype\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21musing_pyarrow_string_dtype\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m---> 56\u001b[0m     _mode_options \u001b[38;5;241m=\u001b[39m \u001b[43m_global_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfuture\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _mode_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfer_string\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Data Manipulation and Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Machine Learning and Data Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Parallel and Asynchronous Programming\n",
    "import multiprocessing\n",
    "import asyncio\n",
    "import joblib\n",
    "\n",
    "# Miscellaneous\n",
    "from scipy import signal\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.input_dir = \"Data/IndicatorData\"\n",
    "        self.output_dir = \"Data/ScaledData\"\n",
    "        self.log_file = \"Data/ScaledData/_ScalingErrors.log\"\n",
    "        self.scaling_methods_file = \"__ScalingMethods.csv\"  # Add this line\n",
    "\n",
    "    def setup_logging(self):\n",
    "        logging.basicConfig(filename=self.log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        logging.info(f\"Logging started at {datetime.datetime.now()}\")\n",
    "\n",
    "###===================================( Data Preprocessing )===================================###\n",
    "###===================================( Data Preprocessing )===================================###\n",
    "###===================================( Data Preprocessing )===================================###\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_scaling_methods(config):\n",
    "    scaling_methods_file = config.scaling_methods_file  # File in the workspace folder\n",
    "    try:\n",
    "        scaling_methods_df = pd.read_csv(scaling_methods_file)\n",
    "        scaling_methods = {}\n",
    "        for _, row in scaling_methods_df.iterrows():\n",
    "            scaling_methods[row['Column']] = {\n",
    "                'ScalingMethod': row['ScalingMethod'],\n",
    "                'LogTransform': row['LogTransform'] == 'True',\n",
    "                'Detrend': row['Detrend']\n",
    "            }\n",
    "        logging.info(\"Successfully loaded scaling methods.\")\n",
    "        return scaling_methods\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading scaling methods: {e}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "\n",
    "def outlyer_squasher(df, percentile1=0.999, percentile2=0.001):\n",
    "    try:\n",
    "        for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "            lower_quantile = df[col].quantile(percentile2)  # Get the lower quantile\n",
    "            upper_quantile = df[col].quantile(percentile1)  # Get the upper quantile\n",
    "            df[col] = df[col].clip(lower=lower_quantile, upper=upper_quantile)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error squashing outliers: {e}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def handle_inf_and_scale_dynamic(df, scaling_methods, window_ratio=0.1, min_window_size=100):\n",
    "    try:\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df.fillna(method='ffill', inplace=True)\n",
    "\n",
    "        window_size = max(int(len(df) * window_ratio), min_window_size)\n",
    "        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.difference(['Date'])\n",
    "\n",
    "        for start in range(0, len(df), window_size):\n",
    "            end = min(start + window_size, len(df))\n",
    "            df_subset = df.iloc[start:end].copy()\n",
    "            \n",
    "            for col in numeric_cols:\n",
    "                if col in scaling_methods and col in df_subset.columns:\n",
    "                    scaling_method = scaling_methods[col]['ScalingMethod']\n",
    "                    \n",
    "                    if scaling_method == \"NoScaling\":\n",
    "                        continue  # Skip scaling for columns with 'None' as the scaling method\n",
    "\n",
    "                    if scaling_methods[col]['LogTransform']:\n",
    "                        df_subset[col] = np.log1p(df_subset[col])\n",
    "                    if scaling_methods[col]['Detrend']:\n",
    "                        df_subset[col] = signal.detrend(df_subset[col].fillna(method='pad'))  # detrend and handle NaNs\n",
    "                    \n",
    "                    scaler = get_scaler(scaling_method)\n",
    "                    if scaler is not None:\n",
    "                        df_subset[col] = scaler.fit_transform(df_subset[[col]].fillna(method='pad'))  # scale and handle NaNs\n",
    "                    else:\n",
    "                        df_subset[col] = df_subset[col].fillna(method='pad')\n",
    "\n",
    "            df.iloc[start:end, df.columns.isin(numeric_cols)] = df_subset\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in dynamic scaling of dataframe: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_scaler(scaling_method):\n",
    "    if scaling_method == 'StandardScaler':\n",
    "        return StandardScaler()\n",
    "    elif scaling_method == 'MinMaxScaler':\n",
    "        return MinMaxScaler()\n",
    "    elif scaling_method == 'RobustScaler':\n",
    "        return RobustScaler()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def apply_log_transform(df, column):\n",
    "    try:\n",
    "        df[column] = np.log1p(df[column])\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error applying log transform to column {column}: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def detrend_data(df, column, method):\n",
    "    try:\n",
    "        if method == 'Differencing':\n",
    "            df[column] = df[column].diff().fillna(df[column])\n",
    "        elif method == 'ScipyDetrend':\n",
    "            df[column] = signal.detrend(df[column])\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error detrending column {column}: {e}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def interpolated(df):\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    numeric_df = numeric_df.interpolate(method='linear', limit_direction='both')\n",
    "    df[numeric_df.columns] = numeric_df\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_dataframe(df, file_path):\n",
    "    config = Config()  # Instantiate a Config object\n",
    "    file_name = file_path.split(\"/\")[-1].split(\"_\")[0] + \"_scaled.csv\"\n",
    "    file_path = os.path.join(config.output_dir, file_name)  # Use the instance's output_dir\n",
    "    df.to_csv(file_path, index=False)\n",
    "    logging.info(f\"File {file_name} has been saved to {config.output_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_dataframe(df, file_path, scaling_methods):\n",
    "    df = outlyer_squasher(df)\n",
    "    df = interpolated(df)\n",
    "    df = handle_inf_and_scale_dynamic(df, scaling_methods)\n",
    "    df = interpolated(df)\n",
    "    if len(df) > 400:\n",
    "\n",
    "        save_dataframe(df, file_path)\n",
    "    else:\n",
    "        logging.error(f\"File {file_path} has less than 400 rows and will not be saved.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = Config()\n",
    "    config.setup_logging()\n",
    "\n",
    "    # Suppress specific warnings\n",
    "    warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "    # Load the scaling methods\n",
    "    scaling_methods = load_scaling_methods(config)\n",
    "\n",
    "    # Clear all files ending in .csv in the output directory\n",
    "    for file in os.listdir(config.output_dir):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(config.output_dir, file)\n",
    "            os.remove(file_path)\n",
    "    logging.info(f\"Files in {config.output_dir} have been cleared.\")\n",
    "\n",
    "    # Process files with tqdm progress bar\n",
    "    for file in tqdm(os.listdir(config.input_dir), desc=\"Processing files\"):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(config.input_dir, file)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                df.shape[0]\n",
    "                if df.shape[0] < 400:\n",
    "                    logging.info(f\"Processing file {file} too short to process.\")\n",
    "                    continue\n",
    "                \n",
    "\n",
    "                process_dataframe(df, file_path, scaling_methods)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
