{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:   0%|          | 0/5751 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files:   0%|          | 0/5751 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'filtered_data' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 295\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    294\u001b[0m     directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData/RFpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 295\u001b[0m     \u001b[43mprocess_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 209\u001b[0m, in \u001b[0;36mprocess_directory\u001b[0;34m(directory, predictions_column, min_distribution_threshold)\u001b[0m\n\u001b[1;32m    207\u001b[0m     skipped_fileCounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m unsure_correct \u001b[38;5;241m=\u001b[39m \u001b[43mfiltered_data\u001b[49m[(filtered_data[predictions_column] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (filtered_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpercent_change_Close\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)]\n\u001b[1;32m    210\u001b[0m total_unsure \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(filtered_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpercent_change_Close\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    211\u001b[0m unsure_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(unsure_correct) \u001b[38;5;241m/\u001b[39m total_unsure \u001b[38;5;28;01mif\u001b[39;00m total_unsure \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'filtered_data' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # Corrected import\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    return data\n",
    "\n",
    "def filter_predictions(data, predictions_column):\n",
    "    return data[data[predictions_column] != 0]\n",
    "\n",
    "def calculate_additional_metrics(data, predictions_column):\n",
    "    def safe_mean(df):\n",
    "        return df.mean() if not df.empty else float('nan')  # or use 0 as a default\n",
    "\n",
    "    correct_up = data[(data[predictions_column] == 1) & (data['percent_change_Close'] > 0)]['percent_change_Close']\n",
    "    correct_down = data[(data[predictions_column] == -1) & (data['percent_change_Close'] < 0)]['percent_change_Close']\n",
    "    wrong_up = data[(data[predictions_column] == 1) & (data['percent_change_Close'] < 0)]['percent_change_Close']\n",
    "    wrong_down = data[(data[predictions_column] == -1) & (data['percent_change_Close'] > 0)]['percent_change_Close']\n",
    "    up_diff = safe_mean(correct_up) - safe_mean(wrong_up)\n",
    "    down_diff = safe_mean(correct_down) - safe_mean(wrong_down)\n",
    "    positive_preds = sum(data[predictions_column] == 1)\n",
    "    negative_preds = sum(data[predictions_column] == -1)\n",
    "    non_predictions = sum(data[predictions_column] == 0)\n",
    "    \n",
    "    return {\n",
    "        \"correct_up_avg\": safe_mean(correct_up) * 100,  # Multiplied by 100\n",
    "        \"wrong_up_avg\": safe_mean(wrong_up) * 100,  # Multiplied by 100\n",
    "        \"up_diff\": up_diff * 100,  # Multiplied by 100\n",
    "        \"correct_down_avg\": safe_mean(correct_down) * 100,  # Multiplied by 100\n",
    "        \"wrong_down_avg\": safe_mean(wrong_down) * 100,  # Multiplied by 100\n",
    "        \"down_diff\": down_diff * 100,  # Multiplied by 100\n",
    "        \"positive_preds\": positive_preds,\n",
    "        \"negative_preds\": negative_preds,\n",
    "        \"non_predictions\": non_predictions\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_distributions(extended_metrics_list):\n",
    "    correct_up_values = [item[\"correct_up_stats\"][\"mean\"] for item in extended_metrics_list if item[\"correct_up_stats\"][\"mean\"] is not float('nan')]\n",
    "    wrong_up_values = [item[\"wrong_up_stats\"][\"mean\"] for item in extended_metrics_list if item[\"wrong_up_stats\"][\"mean\"] is not float('nan')]\n",
    "    correct_down_values = [item[\"correct_down_stats\"][\"mean\"] for item in extended_metrics_list if item[\"correct_down_stats\"][\"mean\"] is not float('nan')]\n",
    "    wrong_down_values = [item[\"wrong_down_stats\"][\"mean\"] for item in extended_metrics_list if item[\"wrong_down_stats\"][\"mean\"] is not float('nan')]\n",
    "    fig = make_subplots(rows=2, cols=2, subplot_titles=(\"Correct Up\", \"Wrong Up\", \"Correct Down\", \"Wrong Down\"))\n",
    "    bin_size = 0.1\n",
    "    fig.add_trace(go.Histogram(x=correct_up_values, xbins=dict(start=np.floor(min(correct_up_values)), end=np.ceil(max(correct_up_values)), size=bin_size), name='Correct Up'), row=1, col=1)\n",
    "    fig.add_trace(go.Histogram(x=wrong_up_values, xbins=dict(start=np.floor(min(wrong_up_values)), end=np.ceil(max(wrong_up_values)), size=bin_size), name='Wrong Up'), row=1, col=2)\n",
    "    fig.add_trace(go.Histogram(x=correct_down_values, xbins=dict(start=np.floor(min(correct_down_values)), end=np.ceil(max(correct_down_values)), size=bin_size), name='Correct Down'), row=2, col=1)\n",
    "    fig.add_trace(go.Histogram(x=wrong_down_values, xbins=dict(start=np.floor(min(wrong_down_values)), end=np.ceil(max(wrong_down_values)), size=bin_size), name='Wrong Down'), row=2, col=2)\n",
    "    fig.update_layout(template=\"plotly_dark\")\n",
    "    fig.update_layout(title_text=\"Distribution of Prediction Metrics\", height=900, width=1200)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def evaluate_magnitude_prediction(data):\n",
    "    data['Absolute_Percent_Change'] = data['percent_change_Close'].abs()\n",
    "    mae = mean_absolute_error(data['Absolute_Percent_Change'], data['MagnitudePrediction'])\n",
    "    rmse = root_mean_squared_error(data['Absolute_Percent_Change'], data['MagnitudePrediction'])\n",
    "    correlation = data[['Absolute_Percent_Change', 'MagnitudePrediction']].corr().iloc[0, 1]\n",
    "    return mae, rmse, correlation\n",
    "\n",
    "\n",
    "def calculate_extended_metrics(data, predictions_column):\n",
    "    def safe_stats(series):\n",
    "        capped_series = series.clip(lower=-2.5, upper=2.5) * 100  # Multiplied by 100\n",
    "        return {\n",
    "            \"mean\": round(capped_series.mean(), 2) if not capped_series.empty else float('nan'),\n",
    "            \"max\": round(capped_series.max(), 2) if not capped_series.empty else float('nan'),\n",
    "            \"min\": round(capped_series.min(), 2) if not capped_series.empty else float('nan'),\n",
    "            \"std\": round(capped_series.std(), 2) if not capped_series.empty else float('nan')\n",
    "        }\n",
    "\n",
    "    correct_up = data[(data[predictions_column] == 1) & (data['percent_change_Close'] > 0)]['percent_change_Close']\n",
    "    correct_down = data[(data[predictions_column] == -1) & (data['percent_change_Close'] < 0)]['percent_change_Close']\n",
    "    wrong_up = data[(data[predictions_column] == 1) & (data['percent_change_Close'] < 0)]['percent_change_Close']\n",
    "    wrong_down = data[(data[predictions_column] == -1) & (data['percent_change_Close'] > 0)]['percent_change_Close']\n",
    "\n",
    "    return {\n",
    "        \"correct_up_stats\": safe_stats(correct_up),\n",
    "        \"correct_down_stats\": safe_stats(correct_down),\n",
    "        \"wrong_up_stats\": safe_stats(wrong_up),\n",
    "        \"wrong_down_stats\": safe_stats(wrong_down),\n",
    "        \"positive_preds\": sum(data[predictions_column] == 1),\n",
    "        \"negative_preds\": sum(data[predictions_column] == -1),\n",
    "        \"non_predictions\": sum(data[predictions_column] == 0)\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model(data, predictions_column):\n",
    "    filtered_data = data[data['percent_change_Close'] != 0]\n",
    "    actual = filtered_data['percent_change_Close'].apply(lambda x: 1 if x > 0 else -1)\n",
    "    predicted = filtered_data[predictions_column]\n",
    "    accuracy = accuracy_score(actual, predicted)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(actual, predicted, average='weighted', zero_division=1)\n",
    "    return accuracy, (precision, recall, f1)\n",
    "\n",
    "\n",
    "def print_average_extended_metrics(extended_metrics_list):\n",
    "    print(\"\\n\")\n",
    "    header = \"{:<20} {:>10} {:>10} {:>10} {:>10}\".format(\"Metric\", \"Mean\", \"Max\", \"Min\", \"Std Dev\")\n",
    "    print(header)\n",
    "    print(\"-\" * 65)  # Separator\n",
    "\n",
    "    for metric in [\"correct_up_stats\", \"wrong_up_stats\", \"correct_down_stats\", \"wrong_down_stats\"]:\n",
    "        mean_metric = {\n",
    "            k: round(np.nanmean([d[metric][k] for d in extended_metrics_list]), 2)\n",
    "            for k in [\"mean\", \"max\", \"min\", \"std\"]\n",
    "        }\n",
    "        row = \"{:<20} {:>10} {:>10} {:>10} {:>10}\".format(\n",
    "            metric,\n",
    "            mean_metric[\"mean\"],\n",
    "            mean_metric[\"max\"],\n",
    "            mean_metric[\"min\"],\n",
    "            mean_metric[\"std\"]\n",
    "        )\n",
    "        print(row)\n",
    "\n",
    "def evaluate_model(data, predictions_column):\n",
    "    label_map = {1: 'Up', 0: 'Unsure', -1: 'Down'}\n",
    "    actual_labels = data['percent_change_Close'].apply(lambda x: 'Up' if x > 0 else ('Down' if x < 0 else 'Unsure'))\n",
    "    predicted_labels = data[predictions_column].map(label_map)\n",
    "    accuracy = accuracy_score(actual_labels, predicted_labels)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(actual_labels, predicted_labels, average='macro', zero_division=0)\n",
    "    cm = confusion_matrix(actual_labels, predicted_labels, labels=[\"Up\", \"Down\", \"Unsure\"])\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    return accuracy, (precision, recall, f1)\n",
    "\n",
    "def process_directory(directory, predictions_column='Prediction', min_distribution_threshold=0.0):\n",
    "    accuracies = []\n",
    "    up_accuracies = []\n",
    "    down_accuracies = []\n",
    "    extended_metrics_list = []\n",
    "    metrics_list = []\n",
    "    detailed_metrics = []\n",
    "    skipped_fileCounter = 0\n",
    "    counted_files = 0\n",
    "    totalfiles = 0\n",
    "    maes, rmses, correlations = [], [], []\n",
    "    unsure_accuracy = []\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "        for filename in tqdm(os.listdir(directory), desc=\"Processing Files\"):\n",
    "            if filename.endswith(\".csv\") and filename != \"log_file_name.csv\":\n",
    "                totalfiles += 1\n",
    "                file_path = os.path.join(directory, filename)\n",
    "                data = load_data(file_path)\n",
    "    \n",
    "                # Skip the first 400 rows\n",
    "                if len(data) > 400:\n",
    "                    data = data.iloc[400:]\n",
    "                else:\n",
    "                    skipped_fileCounter += 1\n",
    "                    continue\n",
    "                unsure_correct = filtered_data[(filtered_data[predictions_column] == 0) & (filtered_data['percent_change_Close'] == 0)]\n",
    "                total_unsure = sum(filtered_data['percent_change_Close'] == 0)\n",
    "                unsure_acc = len(unsure_correct) / total_unsure if total_unsure != 0 else 0\n",
    "                unsure_accuracy.append(unsure_acc)\n",
    "\n",
    "                mae, rmse, corr = evaluate_magnitude_prediction(data)\n",
    "                maes.append(mae)\n",
    "                rmses.append(rmse)\n",
    "                correlations.append(corr)\n",
    "\n",
    "                category_counts = data[predictions_column].value_counts(normalize=True)\n",
    "                if all(count >= min_distribution_threshold for count in category_counts):\n",
    "                    filtered_data = filter_predictions(data, predictions_column)\n",
    "                    accuracy, metrics = evaluate_model(filtered_data, predictions_column)\n",
    "                    accuracies.append(accuracy)\n",
    "                    metrics_list.append(metrics)\n",
    "                    \n",
    "                    # Calculate category-specific accuracies\n",
    "                    up_correct = len(filtered_data[(filtered_data[predictions_column] == 1) & (filtered_data['percent_change_Close'] > 0)])\n",
    "                    down_correct = len(filtered_data[(filtered_data[predictions_column] == -1) & (filtered_data['percent_change_Close'] < 0)])\n",
    "                    unsure_correct = len(filtered_data[(filtered_data[predictions_column] == 0) & (filtered_data['percent_change_Close'] == 0)])\n",
    "                    total_up = sum(filtered_data['percent_change_Close'] > 0)\n",
    "                    total_down = sum(filtered_data['percent_change_Close'] < 0)\n",
    "                    total_unsure = sum(filtered_data['percent_change_Close'] == 0)\n",
    "                    up_accuracies.append(up_correct / total_up if total_up != 0 else 0)\n",
    "                    down_accuracies.append(down_correct / total_down if total_down != 0 else 0)\n",
    "\n",
    "                    extended_metrics = calculate_extended_metrics(filtered_data, predictions_column)\n",
    "                    extended_metrics_list.append(extended_metrics)\n",
    "                    additional_metrics = calculate_additional_metrics(filtered_data, predictions_column)\n",
    "                    detailed_metrics.append(additional_metrics)\n",
    "                    counted_files += 1\n",
    "                else:\n",
    "                    skipped_fileCounter += 1\n",
    "\n",
    "\n",
    "\n",
    "    avg_up_accuracy = np.mean(up_accuracies)\n",
    "    avg_down_accuracy = np.mean(down_accuracies)\n",
    "    avg_mae = np.mean(maes)\n",
    "    avg_rmse = np.mean(rmses)\n",
    "    avg_corr = np.mean(correlations)\n",
    "    print(\"Regressor Model Performance:\")\n",
    "    print(f\"Average MAE across all files: {avg_mae:.3f}\")\n",
    "    print(f\"Average RMSE across all files: {avg_rmse:.3f}\")\n",
    "    print(f\"Average Correlation across all files: {avg_corr:.3f}\")\n",
    "    print(\"Average Classifier Model Performance with Distribution Filtering:\")\n",
    "    print(f\"Average Up Accuracy: {avg_up_accuracy*100:.3f}\")\n",
    "    print(f\"Average Down Accuracy: {avg_down_accuracy*100:.3f}\")\n",
    "    MarketUp = 0.499569\n",
    "    MarketDown = 0.500430\n",
    "    Better_than_Random_Up = avg_up_accuracy - MarketUp\n",
    "    Better_than_Random_Down = avg_down_accuracy - MarketDown\n",
    "    print(f\"Better than Random Down: {Better_than_Random_Down*100:.3f}\")\n",
    "    print(f\"Better than Random Up: {Better_than_Random_Up*100:.3f}\")\n",
    "    print(f\"Total files: {totalfiles}\")\n",
    "    print(f\"Skipped {skipped_fileCounter} files due to inadequate distribution across categories, Processed {counted_files} files.\")\n",
    "    print(f\"Skipped percentage: {skipped_fileCounter/totalfiles*100:.2f}%\")\n",
    "\n",
    "    for metric in [\"correct_up_avg\", \"correct_down_avg\", \"wrong_up_avg\", \"wrong_down_avg\", \"positive_preds\", \"negative_preds\", \"non_predictions\"]:\n",
    "        avg_metric = np.nanmean([d[metric] for d in detailed_metrics])\n",
    "        print(f\"Average {metric}: {avg_metric:.3f}\")\n",
    "\n",
    "    print_average_extended_metrics(extended_metrics_list)\n",
    "    plot_distributions(extended_metrics_list)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory = \"Data/RFpredictions\"\n",
    "    process_directory(directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
